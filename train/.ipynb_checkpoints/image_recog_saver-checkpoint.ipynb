{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.0.1\n",
      "Torchvision Version:  0.2.2\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1060\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "# Top level data directory. Here we assume the format of the directory conforms\n",
    "#   to the ImageFolder structure\n",
    "data_dir = \"/home/ayon/dev/enginx/GroceryStoreDataset/dataset/\"\n",
    "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
    "model_name = \"squeezenet\"\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 15\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size = 32\n",
    "\n",
    "# Number of epochs to train for\n",
    "num_epochs = 200\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model,\n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract = True\n",
    "\n",
    "chk_dir = '/home/ayon/.torch/checkpoint/'\n",
    "\n",
    "global model_hist, start_epoch, is_resume\n",
    "model_hist = {'train':[], 'test':[]}\n",
    "start_epoch = 0\n",
    "is_resume = True\n",
    "\n",
    "saved_path = '/home/ayon/.torch/checkpoint/saved_squeezenet_175_train_0.9751_test_0.95364_2019-05-24_02-42-49.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summ_image(path, text, n_iter, class_i):\n",
    "    %matplotlib inline\n",
    "    im = plt.imread(path)\n",
    "\n",
    "#     fig = plt.figure()\n",
    "#     # plt.clf()\n",
    "#     fig.add_subplot()\n",
    "#     plt.imshow(im)\n",
    "#     plt.tick_params(\n",
    "#         axis='x',          # changes apply to the x-axis\n",
    "#         which='both',      # both major and minor ticks are affected\n",
    "#         bottom=False,      # ticks along the bottom edge are off\n",
    "#         top=False,         # ticks along the top edge are off\n",
    "#         labelbottom=False)\n",
    "#     plt.tick_params(\n",
    "#         axis='y',          # changes apply to the x-axis\n",
    "#         which='both',      # both major and minor ticks are affected\n",
    "#         bottom=False,      # ticks along the bottom edge are off\n",
    "#         top=False,         # ticks along the top edge are off\n",
    "#         labelleft=False)\n",
    "#     t = plt.text(30, 50, text, fontsize=12)\n",
    "#     t.set_bbox(dict(facecolor='white', alpha=0.8, edgecolor='red'))\n",
    "#     fig.tight_layout(pad=0)\n",
    "# #     plt.show()\n",
    "#     fig.canvas.draw()\n",
    "\n",
    "#     # Now we can save it to a numpy array.\n",
    "#     data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "#     data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "#     data = data.transpose((2,0,1))\n",
    "    \n",
    "    writer.add_image(str(class_i), im.transpose((2,0,1)))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False, save_interval = 5):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    \n",
    "#     if start_epoch > 0:\n",
    "#         print('Start epoch :{}'.format(start_epoch))\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, paths in dataloaders[phase]:\n",
    "#                 print(\"Phase: \", phase)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        \n",
    "#                     if phase == 'test':\n",
    "#                         l = int(preds.cpu().numpy())\n",
    "#                         summ_image(paths[0], test_class_dict[l], epoch, test_class_dict[l])\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = float(epoch_acc.cpu().numpy())\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            model_hist[phase].append(( start_epoch + epoch, epoch_loss, epoch_acc)) ## continue from saved epoch\n",
    "            \n",
    "            # deep copy the model\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "#                 best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'test':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                \n",
    "            \n",
    "            if epoch%save_interval == 0 and phase == 'test':\n",
    "                save_checkpoint(os.path.join(chk_dir, 'saved'),\n",
    "                                   metric_dict={\n",
    "                                       'epoch': model_hist['train'][-1][0],\n",
    "                                       'type': model_name,\n",
    "                                       'train_acc': model_hist['train'][-1][2],\n",
    "                                       'test_acc': model_hist['test'][-1][2],\n",
    "                                       'model_dict': model_ft.state_dict()\n",
    "                                   })\n",
    "#                 for name, param in model.named_parameters():\n",
    "#                     writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
    "            \n",
    "            if phase == 'test':\n",
    "                writer.add_scalars('data/scalars', {\n",
    "                'train_loss': model_hist['train'][-1][1],\n",
    "                'train_acc': model_hist['train'][-1][2],\n",
    "                'test_acc': model_hist['test'][-1][2]\n",
    "                })                    \n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model(model, dataloaders, criterion, optimizer, num_epochs=1, is_inception=False, save_interval = 5):\n",
    "    since = time.time()\n",
    "\n",
    "#     val_acc_history = []\n",
    "    \n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['test']:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels, paths in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "\n",
    "                if phase == 'test':\n",
    "                    l = int(preds.cpu().numpy())\n",
    "                    summ_image(paths[0], test_class_dict[l], epoch, test_class_dict[l])\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = float(epoch_acc.cpu().numpy())\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            model_hist[phase].append(( start_epoch + epoch, epoch_loss, epoch_acc)) ## continue from saved epoch\n",
    "            \n",
    "            if phase == 'test':\n",
    "                writer.add_scalars('test/scalars', {\n",
    "                'test_acc': model_hist['test'][-1][2]\n",
    "                })                    \n",
    "            \n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "#     print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "#     model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n",
    "    # Initialize these variables which will be set in this if statement. Each of these\n",
    "    #   variables is model specific.\n",
    "    model_ft = None\n",
    "    input_size = 0\n",
    "\n",
    "    if model_name == \"resnet\":\n",
    "        \"\"\" Resnet18\n",
    "        \"\"\"\n",
    "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"alexnet\":\n",
    "        \"\"\" Alexnet\n",
    "        \"\"\"\n",
    "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"vgg\":\n",
    "        \"\"\" VGG16_bn\n",
    "        \"\"\"\n",
    "        model_ft = models.vgg16_bn(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier[6].in_features\n",
    "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"squeezenet\":\n",
    "        \"\"\" Squeezenet\n",
    "        \"\"\"\n",
    "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
    "        model_ft.num_classes = num_classes\n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"densenet\":\n",
    "        \"\"\" Densenet\n",
    "        \"\"\"\n",
    "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        num_ftrs = model_ft.classifier.in_features\n",
    "        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n",
    "        input_size = 224\n",
    "\n",
    "    elif model_name == \"inception\":\n",
    "        \"\"\" Inception v3 \n",
    "        Be careful, expects (299,299) sized images and has auxiliary output\n",
    "        \"\"\"\n",
    "        model_ft = models.inception_v3(pretrained=use_pretrained)\n",
    "        set_parameter_requires_grad(model_ft, feature_extract)\n",
    "        # Handle the auxilary net\n",
    "        num_ftrs = model_ft.AuxLogits.fc.in_features\n",
    "        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
    "        # Handle the primary net\n",
    "        num_ftrs = model_ft.fc.in_features\n",
    "        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n",
    "        input_size = 299\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid model name, exiting...\")\n",
    "        exit()\n",
    "    \n",
    "    return model_ft, input_size\n",
    "\n",
    "# Initialize the model for this run\n",
    "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
    "\n",
    "# Print the model we just instantiated\n",
    "# print(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Datasets and Dataloaders...\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation((-30, 30)),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Initializing Datasets and Dataloaders...\")\n",
    "\n",
    "class ImageFolderWithPaths(datasets.ImageFolder):\n",
    "    \"\"\"Custom dataset that includes image file paths. Extends\n",
    "    torchvision.datasets.ImageFolder\n",
    "    \"\"\"\n",
    "\n",
    "    # override the __getitem__ method. this is the method dataloader calls\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns \n",
    "        original_tuple = super(ImageFolderWithPaths, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (original_tuple + (path,))\n",
    "        return tuple_with_path\n",
    "\n",
    "# Create training and validation datasets\n",
    "image_datasets = {x: ImageFolderWithPaths(os.path.join(data_dir, x, 'Root'), data_transforms[x]) for x in ['train'\n",
    "                                                                                                           , 'test'\n",
    "                                                                                                                ]}\n",
    "# Create training and validation dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(image_datasets['train'], batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(image_datasets['test'], batch_size=1, shuffle=True, num_workers=4)\n",
    "# dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train'\n",
    "#                                                                                                                                     , 'test'\n",
    "#                                                                                                                                    ]}\n",
    "test_class_dict = {value: key for key, value in testloader.dataset.class_to_idx.items()}\n",
    "dataloaders_dict = {'train': trainloader, 'test': testloader}\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saver Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_pat = re.compile(\n",
    "            \"^(\\d{4}-\\d{2}-\\d{2})\\s(\\d{2}:\\d{2}:\\d{2})?\")\n",
    "def get_datetime():  # generates date-time for logging\n",
    "    match = date_pat.search(str(datetime.now()))\n",
    "    date, time = match.group(1), match.group(2)\n",
    "    return date, time\n",
    "\n",
    "# def json2dict(file):  # reads saved json files\n",
    "#     assert (osp.isfile(file)), \\\n",
    "#         (\"JSON file provided doesn't exist! Dictionary is NoneType.\")\n",
    "#     dic = {}\n",
    "#     with open(file, 'r') as f:\n",
    "#         dic = json.load(f)\n",
    "#     return dic\n",
    "\n",
    "def save_checkpoint(path=None, metric_dict=None, mode='train'):\n",
    "\n",
    "    date, time = get_datetime()\n",
    "    time = time.replace(':', '-')\n",
    "    print(date, time)\n",
    "    chk_name = '{}_{}_{}_train_{}_test_{}_{}_{}.pth'.format(\n",
    "        path,\n",
    "        metric_dict['type'],\n",
    "        metric_dict['epoch'],\n",
    "        round(metric_dict['train_acc'], 5),\n",
    "        round(metric_dict['test_acc'], 5),\n",
    "        date,\n",
    "        time\n",
    "    )\n",
    "    torch.save(metric_dict, chk_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load old model / Init new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  175\n",
      "train_acc :  0.9750982961992136\n",
      "test_acc :  0.9536423841059603\n"
     ]
    }
   ],
   "source": [
    "chk_name = saved_path\n",
    "if not os.path.isfile(saved_path):\n",
    "    chk_path = os.path.join(chk_dir, chk_name)\n",
    "else:\n",
    "    chk_path = saved_path\n",
    "    \n",
    "if is_resume:\n",
    "    loaded =  torch.load(chk_path)\n",
    "    [print(x, \": \", loaded[x]) for x in ['epoch', 'train_acc', 'test_acc']];\n",
    "\n",
    "    #### load saved model?\n",
    "\n",
    "    model_ft.load_state_dict(loaded['model_dict'])\n",
    "    start_epoch = loaded['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Params to learn:\n",
      "\t classifier.1.weight\n",
      "\t classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "# Send the model to GPU\n",
    "print(device)\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Gather the parameters to be optimized/updated in this run. If we are\n",
    "#  finetuning we will be updating all parameters. However, if we are \n",
    "#  doing feature extract method, we will only update the parameters\n",
    "#  that we have just initialized, i.e. the parameters with requires_grad\n",
    "#  is True.\n",
    "\n",
    "params_to_update = model_ft.parameters()\n",
    "print(\"Params to learn:\")\n",
    "if feature_extract:\n",
    "    params_to_update = []\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "            print(\"\\t\",name)\n",
    "else:\n",
    "    for name,param in model_ft.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            print(\"\\t\",name)\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "train Loss: 0.0777 Acc: 0.9817\n",
      "test Loss: 0.1925 Acc: 0.9404\n",
      "2019-05-24 21-03-41\n",
      "\n",
      "Epoch 1/199\n",
      "----------\n",
      "train Loss: 0.0840 Acc: 0.9803\n",
      "test Loss: 0.1958 Acc: 0.9448\n",
      "\n",
      "Epoch 2/199\n",
      "----------\n",
      "train Loss: 0.0769 Acc: 0.9817\n",
      "test Loss: 0.1883 Acc: 0.9448\n",
      "\n",
      "Epoch 3/199\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9790\n",
      "test Loss: 0.1971 Acc: 0.9448\n",
      "\n",
      "Epoch 4/199\n",
      "----------\n",
      "train Loss: 0.0641 Acc: 0.9803\n",
      "test Loss: 0.1775 Acc: 0.9470\n",
      "\n",
      "Epoch 5/199\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9751\n",
      "test Loss: 0.1788 Acc: 0.9514\n",
      "2019-05-24 21-04-02\n",
      "\n",
      "Epoch 6/199\n",
      "----------\n",
      "train Loss: 0.0677 Acc: 0.9817\n",
      "test Loss: 0.1906 Acc: 0.9514\n",
      "\n",
      "Epoch 7/199\n",
      "----------\n",
      "train Loss: 0.0634 Acc: 0.9803\n",
      "test Loss: 0.1828 Acc: 0.9536\n",
      "\n",
      "Epoch 8/199\n",
      "----------\n",
      "train Loss: 0.0771 Acc: 0.9817\n",
      "test Loss: 0.1811 Acc: 0.9514\n",
      "\n",
      "Epoch 9/199\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9803\n",
      "test Loss: 0.1995 Acc: 0.9470\n",
      "\n",
      "Epoch 10/199\n",
      "----------\n",
      "train Loss: 0.0880 Acc: 0.9777\n",
      "test Loss: 0.2049 Acc: 0.9492\n",
      "2019-05-24 21-04-23\n",
      "\n",
      "Epoch 11/199\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9738\n",
      "test Loss: 0.1898 Acc: 0.9470\n",
      "\n",
      "Epoch 12/199\n",
      "----------\n",
      "train Loss: 0.0742 Acc: 0.9817\n",
      "test Loss: 0.1772 Acc: 0.9470\n",
      "\n",
      "Epoch 13/199\n",
      "----------\n",
      "train Loss: 0.0688 Acc: 0.9764\n",
      "test Loss: 0.1808 Acc: 0.9492\n",
      "\n",
      "Epoch 14/199\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9817\n",
      "test Loss: 0.1909 Acc: 0.9448\n",
      "\n",
      "Epoch 15/199\n",
      "----------\n",
      "train Loss: 0.0701 Acc: 0.9790\n",
      "test Loss: 0.1713 Acc: 0.9492\n",
      "2019-05-24 21-04-44\n",
      "\n",
      "Epoch 16/199\n",
      "----------\n",
      "train Loss: 0.0605 Acc: 0.9817\n",
      "test Loss: 0.1756 Acc: 0.9514\n",
      "\n",
      "Epoch 17/199\n",
      "----------\n",
      "train Loss: 0.0595 Acc: 0.9843\n",
      "test Loss: 0.1762 Acc: 0.9514\n",
      "\n",
      "Epoch 18/199\n",
      "----------\n",
      "train Loss: 0.0756 Acc: 0.9777\n",
      "test Loss: 0.2057 Acc: 0.9448\n",
      "\n",
      "Epoch 19/199\n",
      "----------\n",
      "train Loss: 0.0575 Acc: 0.9803\n",
      "test Loss: 0.2221 Acc: 0.9382\n",
      "\n",
      "Epoch 20/199\n",
      "----------\n",
      "train Loss: 0.0692 Acc: 0.9751\n",
      "test Loss: 0.1963 Acc: 0.9426\n",
      "2019-05-24 21-05-06\n",
      "\n",
      "Epoch 21/199\n",
      "----------\n",
      "train Loss: 0.0740 Acc: 0.9869\n",
      "test Loss: 0.1972 Acc: 0.9470\n",
      "\n",
      "Epoch 22/199\n",
      "----------\n",
      "train Loss: 0.0714 Acc: 0.9830\n",
      "test Loss: 0.1850 Acc: 0.9514\n",
      "\n",
      "Epoch 23/199\n",
      "----------\n",
      "train Loss: 0.0531 Acc: 0.9882\n",
      "test Loss: 0.1998 Acc: 0.9558\n",
      "\n",
      "Epoch 24/199\n",
      "----------\n",
      "train Loss: 0.0593 Acc: 0.9882\n",
      "test Loss: 0.1873 Acc: 0.9514\n",
      "\n",
      "Epoch 25/199\n",
      "----------\n",
      "train Loss: 0.0885 Acc: 0.9790\n",
      "test Loss: 0.1856 Acc: 0.9558\n",
      "2019-05-24 21-05-27\n",
      "\n",
      "Epoch 26/199\n",
      "----------\n",
      "train Loss: 0.0690 Acc: 0.9764\n",
      "test Loss: 0.1691 Acc: 0.9536\n",
      "\n",
      "Epoch 27/199\n",
      "----------\n",
      "train Loss: 0.0625 Acc: 0.9817\n",
      "test Loss: 0.1987 Acc: 0.9470\n",
      "\n",
      "Epoch 28/199\n",
      "----------\n",
      "train Loss: 0.0689 Acc: 0.9790\n",
      "test Loss: 0.2021 Acc: 0.9492\n",
      "\n",
      "Epoch 29/199\n",
      "----------\n",
      "train Loss: 0.0659 Acc: 0.9777\n",
      "test Loss: 0.1985 Acc: 0.9470\n",
      "\n",
      "Epoch 30/199\n",
      "----------\n",
      "train Loss: 0.0636 Acc: 0.9803\n",
      "test Loss: 0.2045 Acc: 0.9426\n",
      "2019-05-24 21-05-48\n",
      "\n",
      "Epoch 31/199\n",
      "----------\n",
      "train Loss: 0.0489 Acc: 0.9882\n",
      "test Loss: 0.2038 Acc: 0.9470\n",
      "\n",
      "Epoch 32/199\n",
      "----------\n",
      "train Loss: 0.0794 Acc: 0.9777\n",
      "test Loss: 0.2081 Acc: 0.9448\n",
      "\n",
      "Epoch 33/199\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9830\n",
      "test Loss: 0.1930 Acc: 0.9426\n",
      "\n",
      "Epoch 34/199\n",
      "----------\n",
      "train Loss: 0.0703 Acc: 0.9803\n",
      "test Loss: 0.1844 Acc: 0.9382\n",
      "\n",
      "Epoch 35/199\n",
      "----------\n",
      "train Loss: 0.0725 Acc: 0.9817\n",
      "test Loss: 0.1715 Acc: 0.9558\n",
      "2019-05-24 21-06-10\n",
      "\n",
      "Epoch 36/199\n",
      "----------\n",
      "train Loss: 0.0577 Acc: 0.9856\n",
      "test Loss: 0.1953 Acc: 0.9470\n",
      "\n",
      "Epoch 37/199\n",
      "----------\n",
      "train Loss: 0.0511 Acc: 0.9882\n",
      "test Loss: 0.1860 Acc: 0.9514\n",
      "\n",
      "Epoch 38/199\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9830\n",
      "test Loss: 0.1809 Acc: 0.9536\n",
      "\n",
      "Epoch 39/199\n",
      "----------\n",
      "train Loss: 0.0606 Acc: 0.9856\n",
      "test Loss: 0.1984 Acc: 0.9536\n",
      "\n",
      "Epoch 40/199\n",
      "----------\n",
      "train Loss: 0.0617 Acc: 0.9817\n",
      "test Loss: 0.1892 Acc: 0.9492\n",
      "2019-05-24 21-06-31\n",
      "\n",
      "Epoch 41/199\n",
      "----------\n",
      "train Loss: 0.0883 Acc: 0.9738\n",
      "test Loss: 0.2148 Acc: 0.9294\n",
      "\n",
      "Epoch 42/199\n",
      "----------\n",
      "train Loss: 0.0630 Acc: 0.9817\n",
      "test Loss: 0.1751 Acc: 0.9581\n",
      "\n",
      "Epoch 43/199\n",
      "----------\n",
      "train Loss: 0.0662 Acc: 0.9843\n",
      "test Loss: 0.1626 Acc: 0.9514\n",
      "\n",
      "Epoch 44/199\n",
      "----------\n",
      "train Loss: 0.0778 Acc: 0.9738\n",
      "test Loss: 0.1731 Acc: 0.9514\n",
      "\n",
      "Epoch 45/199\n",
      "----------\n",
      "train Loss: 0.0855 Acc: 0.9817\n",
      "test Loss: 0.1891 Acc: 0.9558\n",
      "2019-05-24 21-06-52\n",
      "\n",
      "Epoch 46/199\n",
      "----------\n",
      "train Loss: 0.0606 Acc: 0.9843\n",
      "test Loss: 0.1856 Acc: 0.9492\n",
      "\n",
      "Epoch 47/199\n",
      "----------\n",
      "train Loss: 0.0451 Acc: 0.9908\n",
      "test Loss: 0.1806 Acc: 0.9536\n",
      "\n",
      "Epoch 48/199\n",
      "----------\n",
      "train Loss: 0.0544 Acc: 0.9856\n",
      "test Loss: 0.1729 Acc: 0.9536\n",
      "\n",
      "Epoch 49/199\n",
      "----------\n",
      "train Loss: 0.0767 Acc: 0.9817\n",
      "test Loss: 0.2026 Acc: 0.9448\n",
      "\n",
      "Epoch 50/199\n",
      "----------\n",
      "train Loss: 0.0379 Acc: 0.9961\n",
      "test Loss: 0.1734 Acc: 0.9514\n",
      "2019-05-24 21-07-15\n",
      "\n",
      "Epoch 51/199\n",
      "----------\n",
      "train Loss: 0.0594 Acc: 0.9803\n",
      "test Loss: 0.1855 Acc: 0.9360\n",
      "\n",
      "Epoch 52/199\n",
      "----------\n",
      "train Loss: 0.0679 Acc: 0.9803\n",
      "test Loss: 0.1765 Acc: 0.9492\n",
      "\n",
      "Epoch 53/199\n",
      "----------\n",
      "train Loss: 0.0553 Acc: 0.9843\n",
      "test Loss: 0.1970 Acc: 0.9470\n",
      "\n",
      "Epoch 54/199\n",
      "----------\n",
      "train Loss: 0.0513 Acc: 0.9843\n",
      "test Loss: 0.1857 Acc: 0.9470\n",
      "\n",
      "Epoch 55/199\n",
      "----------\n",
      "train Loss: 0.0529 Acc: 0.9843\n",
      "test Loss: 0.1791 Acc: 0.9470\n",
      "2019-05-24 21-07-38\n",
      "\n",
      "Epoch 56/199\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9830\n",
      "test Loss: 0.1809 Acc: 0.9448\n",
      "\n",
      "Epoch 57/199\n",
      "----------\n",
      "train Loss: 0.0683 Acc: 0.9843\n",
      "test Loss: 0.1852 Acc: 0.9492\n",
      "\n",
      "Epoch 58/199\n",
      "----------\n",
      "train Loss: 0.0485 Acc: 0.9869\n",
      "test Loss: 0.1777 Acc: 0.9536\n",
      "\n",
      "Epoch 59/199\n",
      "----------\n",
      "train Loss: 0.0564 Acc: 0.9856\n",
      "test Loss: 0.1779 Acc: 0.9492\n",
      "\n",
      "Epoch 60/199\n",
      "----------\n",
      "train Loss: 0.0700 Acc: 0.9790\n",
      "test Loss: 0.1887 Acc: 0.9470\n",
      "2019-05-24 21-08-01\n",
      "\n",
      "Epoch 61/199\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.9803\n",
      "test Loss: 0.1803 Acc: 0.9404\n",
      "\n",
      "Epoch 62/199\n",
      "----------\n",
      "train Loss: 0.0868 Acc: 0.9738\n",
      "test Loss: 0.1759 Acc: 0.9514\n",
      "\n",
      "Epoch 63/199\n",
      "----------\n",
      "train Loss: 0.1006 Acc: 0.9725\n",
      "test Loss: 0.1886 Acc: 0.9448\n",
      "\n",
      "Epoch 64/199\n",
      "----------\n",
      "train Loss: 0.0475 Acc: 0.9869\n",
      "test Loss: 0.1863 Acc: 0.9470\n",
      "\n",
      "Epoch 65/199\n",
      "----------\n",
      "train Loss: 0.0727 Acc: 0.9751\n",
      "test Loss: 0.1736 Acc: 0.9536\n",
      "2019-05-24 21-08-23\n",
      "\n",
      "Epoch 66/199\n",
      "----------\n",
      "train Loss: 0.0498 Acc: 0.9895\n",
      "test Loss: 0.1800 Acc: 0.9448\n",
      "\n",
      "Epoch 67/199\n",
      "----------\n",
      "train Loss: 0.0760 Acc: 0.9790\n",
      "test Loss: 0.1605 Acc: 0.9536\n",
      "\n",
      "Epoch 68/199\n",
      "----------\n",
      "train Loss: 0.0716 Acc: 0.9777\n",
      "test Loss: 0.1834 Acc: 0.9404\n",
      "\n",
      "Epoch 69/199\n",
      "----------\n",
      "train Loss: 0.0376 Acc: 0.9921\n",
      "test Loss: 0.1817 Acc: 0.9470\n",
      "\n",
      "Epoch 70/199\n",
      "----------\n",
      "train Loss: 0.0548 Acc: 0.9869\n",
      "test Loss: 0.1827 Acc: 0.9492\n",
      "2019-05-24 21-08-45\n",
      "\n",
      "Epoch 71/199\n",
      "----------\n",
      "train Loss: 0.0823 Acc: 0.9803\n",
      "test Loss: 0.1852 Acc: 0.9492\n",
      "\n",
      "Epoch 72/199\n",
      "----------\n",
      "train Loss: 0.0804 Acc: 0.9790\n",
      "test Loss: 0.1550 Acc: 0.9558\n",
      "\n",
      "Epoch 73/199\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9764\n",
      "test Loss: 0.1570 Acc: 0.9603\n",
      "\n",
      "Epoch 74/199\n",
      "----------\n",
      "train Loss: 0.0726 Acc: 0.9790\n",
      "test Loss: 0.1729 Acc: 0.9492\n",
      "\n",
      "Epoch 75/199\n",
      "----------\n",
      "train Loss: 0.0688 Acc: 0.9843\n",
      "test Loss: 0.1717 Acc: 0.9514\n",
      "2019-05-24 21-09-07\n",
      "\n",
      "Epoch 76/199\n",
      "----------\n",
      "train Loss: 0.0595 Acc: 0.9830\n",
      "test Loss: 0.1785 Acc: 0.9514\n",
      "\n",
      "Epoch 77/199\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9830\n",
      "test Loss: 0.1769 Acc: 0.9536\n",
      "\n",
      "Epoch 78/199\n",
      "----------\n",
      "train Loss: 0.0572 Acc: 0.9803\n",
      "test Loss: 0.1968 Acc: 0.9448\n",
      "\n",
      "Epoch 79/199\n",
      "----------\n",
      "train Loss: 0.0510 Acc: 0.9856\n",
      "test Loss: 0.1941 Acc: 0.9536\n",
      "\n",
      "Epoch 80/199\n",
      "----------\n",
      "train Loss: 0.0562 Acc: 0.9895\n",
      "test Loss: 0.1825 Acc: 0.9470\n",
      "2019-05-24 21-09-30\n",
      "\n",
      "Epoch 81/199\n",
      "----------\n",
      "train Loss: 0.0751 Acc: 0.9790\n",
      "test Loss: 0.1798 Acc: 0.9448\n",
      "\n",
      "Epoch 82/199\n",
      "----------\n",
      "train Loss: 0.0615 Acc: 0.9895\n",
      "test Loss: 0.1814 Acc: 0.9492\n",
      "\n",
      "Epoch 83/199\n",
      "----------\n",
      "train Loss: 0.0507 Acc: 0.9856\n",
      "test Loss: 0.1979 Acc: 0.9448\n",
      "\n",
      "Epoch 84/199\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.9803\n",
      "test Loss: 0.1735 Acc: 0.9514\n",
      "\n",
      "Epoch 85/199\n",
      "----------\n",
      "train Loss: 0.0554 Acc: 0.9764\n",
      "test Loss: 0.1862 Acc: 0.9448\n",
      "2019-05-24 21-09-57\n",
      "\n",
      "Epoch 86/199\n",
      "----------\n",
      "train Loss: 0.0529 Acc: 0.9830\n",
      "test Loss: 0.2044 Acc: 0.9448\n",
      "\n",
      "Epoch 87/199\n",
      "----------\n",
      "train Loss: 0.0586 Acc: 0.9843\n",
      "test Loss: 0.1766 Acc: 0.9514\n",
      "\n",
      "Epoch 88/199\n",
      "----------\n",
      "train Loss: 0.0539 Acc: 0.9817\n",
      "test Loss: 0.1897 Acc: 0.9514\n",
      "\n",
      "Epoch 89/199\n",
      "----------\n",
      "train Loss: 0.0584 Acc: 0.9856\n",
      "test Loss: 0.1773 Acc: 0.9558\n",
      "\n",
      "Epoch 90/199\n",
      "----------\n",
      "train Loss: 0.0636 Acc: 0.9843\n",
      "test Loss: 0.1772 Acc: 0.9581\n",
      "2019-05-24 21-10-22\n",
      "\n",
      "Epoch 91/199\n",
      "----------\n",
      "train Loss: 0.0433 Acc: 0.9882\n",
      "test Loss: 0.1797 Acc: 0.9470\n",
      "\n",
      "Epoch 92/199\n",
      "----------\n",
      "train Loss: 0.0486 Acc: 0.9856\n",
      "test Loss: 0.1989 Acc: 0.9404\n",
      "\n",
      "Epoch 93/199\n",
      "----------\n",
      "train Loss: 0.0766 Acc: 0.9764\n",
      "test Loss: 0.1700 Acc: 0.9581\n",
      "\n",
      "Epoch 94/199\n",
      "----------\n",
      "train Loss: 0.0580 Acc: 0.9843\n",
      "test Loss: 0.1732 Acc: 0.9492\n",
      "\n",
      "Epoch 95/199\n",
      "----------\n",
      "train Loss: 0.0570 Acc: 0.9843\n",
      "test Loss: 0.1982 Acc: 0.9426\n",
      "2019-05-24 21-10-46\n",
      "\n",
      "Epoch 96/199\n",
      "----------\n",
      "train Loss: 0.0783 Acc: 0.9738\n",
      "test Loss: 0.1899 Acc: 0.9536\n",
      "\n",
      "Epoch 97/199\n",
      "----------\n",
      "train Loss: 0.0631 Acc: 0.9830\n",
      "test Loss: 0.1816 Acc: 0.9558\n",
      "\n",
      "Epoch 98/199\n",
      "----------\n",
      "train Loss: 0.0749 Acc: 0.9738\n",
      "test Loss: 0.1747 Acc: 0.9514\n",
      "\n",
      "Epoch 99/199\n",
      "----------\n",
      "train Loss: 0.0658 Acc: 0.9817\n",
      "test Loss: 0.1758 Acc: 0.9492\n",
      "\n",
      "Epoch 100/199\n",
      "----------\n",
      "train Loss: 0.0855 Acc: 0.9777\n",
      "test Loss: 0.1690 Acc: 0.9558\n",
      "2019-05-24 21-11-11\n",
      "\n",
      "Epoch 101/199\n",
      "----------\n",
      "train Loss: 0.0660 Acc: 0.9751\n",
      "test Loss: 0.1782 Acc: 0.9514\n",
      "\n",
      "Epoch 102/199\n",
      "----------\n",
      "train Loss: 0.0551 Acc: 0.9830\n",
      "test Loss: 0.1898 Acc: 0.9360\n",
      "\n",
      "Epoch 103/199\n",
      "----------\n",
      "train Loss: 0.0709 Acc: 0.9790\n",
      "test Loss: 0.1627 Acc: 0.9492\n",
      "\n",
      "Epoch 104/199\n",
      "----------\n",
      "train Loss: 0.0572 Acc: 0.9843\n",
      "test Loss: 0.1755 Acc: 0.9514\n",
      "\n",
      "Epoch 105/199\n",
      "----------\n",
      "train Loss: 0.0516 Acc: 0.9856\n",
      "test Loss: 0.1821 Acc: 0.9514\n",
      "2019-05-24 21-11-36\n",
      "\n",
      "Epoch 106/199\n",
      "----------\n",
      "train Loss: 0.0477 Acc: 0.9895\n",
      "test Loss: 0.2050 Acc: 0.9360\n",
      "\n",
      "Epoch 107/199\n",
      "----------\n",
      "train Loss: 0.0573 Acc: 0.9856\n",
      "test Loss: 0.1934 Acc: 0.9426\n",
      "\n",
      "Epoch 108/199\n",
      "----------\n",
      "train Loss: 0.0410 Acc: 0.9908\n",
      "test Loss: 0.1878 Acc: 0.9492\n",
      "\n",
      "Epoch 109/199\n",
      "----------\n",
      "train Loss: 0.0583 Acc: 0.9830\n",
      "test Loss: 0.1864 Acc: 0.9470\n",
      "\n",
      "Epoch 110/199\n",
      "----------\n",
      "train Loss: 0.0650 Acc: 0.9830\n",
      "test Loss: 0.1666 Acc: 0.9492\n",
      "2019-05-24 21-12-01\n",
      "\n",
      "Epoch 111/199\n",
      "----------\n",
      "train Loss: 0.0928 Acc: 0.9843\n",
      "test Loss: 0.1647 Acc: 0.9558\n",
      "\n",
      "Epoch 112/199\n",
      "----------\n",
      "train Loss: 0.0468 Acc: 0.9882\n",
      "test Loss: 0.1628 Acc: 0.9514\n",
      "\n",
      "Epoch 113/199\n",
      "----------\n",
      "train Loss: 0.0701 Acc: 0.9738\n",
      "test Loss: 0.1871 Acc: 0.9514\n",
      "\n",
      "Epoch 114/199\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9817\n",
      "test Loss: 0.1864 Acc: 0.9514\n",
      "\n",
      "Epoch 115/199\n",
      "----------\n",
      "train Loss: 0.0543 Acc: 0.9856\n",
      "test Loss: 0.1751 Acc: 0.9514\n",
      "2019-05-24 21-12-26\n",
      "\n",
      "Epoch 116/199\n",
      "----------\n",
      "train Loss: 0.0430 Acc: 0.9869\n",
      "test Loss: 0.1866 Acc: 0.9448\n",
      "\n",
      "Epoch 117/199\n",
      "----------\n",
      "train Loss: 0.0768 Acc: 0.9843\n",
      "test Loss: 0.1620 Acc: 0.9581\n",
      "\n",
      "Epoch 118/199\n",
      "----------\n",
      "train Loss: 0.0517 Acc: 0.9869\n",
      "test Loss: 0.1723 Acc: 0.9514\n",
      "\n",
      "Epoch 119/199\n",
      "----------\n",
      "train Loss: 0.0619 Acc: 0.9830\n",
      "test Loss: 0.1872 Acc: 0.9492\n",
      "\n",
      "Epoch 120/199\n",
      "----------\n",
      "train Loss: 0.0362 Acc: 0.9934\n",
      "test Loss: 0.1783 Acc: 0.9492\n",
      "2019-05-24 21-12-51\n",
      "\n",
      "Epoch 121/199\n",
      "----------\n",
      "train Loss: 0.0551 Acc: 0.9803\n",
      "test Loss: 0.1714 Acc: 0.9536\n",
      "\n",
      "Epoch 122/199\n",
      "----------\n",
      "train Loss: 0.0671 Acc: 0.9830\n",
      "test Loss: 0.1728 Acc: 0.9603\n",
      "\n",
      "Epoch 123/199\n",
      "----------\n",
      "train Loss: 0.0541 Acc: 0.9830\n",
      "test Loss: 0.1745 Acc: 0.9536\n",
      "\n",
      "Epoch 124/199\n",
      "----------\n",
      "train Loss: 0.0682 Acc: 0.9843\n",
      "test Loss: 0.1787 Acc: 0.9558\n",
      "\n",
      "Epoch 125/199\n",
      "----------\n",
      "train Loss: 0.0558 Acc: 0.9817\n",
      "test Loss: 0.1808 Acc: 0.9514\n",
      "2019-05-24 21-13-15\n",
      "\n",
      "Epoch 126/199\n",
      "----------\n",
      "train Loss: 0.0464 Acc: 0.9869\n",
      "test Loss: 0.1980 Acc: 0.9492\n",
      "\n",
      "Epoch 127/199\n",
      "----------\n",
      "train Loss: 0.0528 Acc: 0.9817\n",
      "test Loss: 0.1929 Acc: 0.9448\n",
      "\n",
      "Epoch 128/199\n",
      "----------\n",
      "train Loss: 0.1085 Acc: 0.9699\n",
      "test Loss: 0.1898 Acc: 0.9514\n",
      "\n",
      "Epoch 129/199\n",
      "----------\n",
      "train Loss: 0.0438 Acc: 0.9856\n",
      "test Loss: 0.1622 Acc: 0.9492\n",
      "\n",
      "Epoch 130/199\n",
      "----------\n",
      "train Loss: 0.0574 Acc: 0.9830\n",
      "test Loss: 0.1759 Acc: 0.9514\n",
      "2019-05-24 21-13-40\n",
      "\n",
      "Epoch 131/199\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9777\n",
      "test Loss: 0.1659 Acc: 0.9514\n",
      "\n",
      "Epoch 132/199\n",
      "----------\n",
      "train Loss: 0.0626 Acc: 0.9817\n",
      "test Loss: 0.1540 Acc: 0.9581\n",
      "\n",
      "Epoch 133/199\n",
      "----------\n",
      "train Loss: 0.0633 Acc: 0.9830\n",
      "test Loss: 0.1804 Acc: 0.9558\n",
      "\n",
      "Epoch 134/199\n",
      "----------\n",
      "train Loss: 0.0825 Acc: 0.9725\n",
      "test Loss: 0.1943 Acc: 0.9448\n",
      "\n",
      "Epoch 135/199\n",
      "----------\n",
      "train Loss: 0.0515 Acc: 0.9830\n",
      "test Loss: 0.2287 Acc: 0.9316\n",
      "2019-05-24 21-14-05\n",
      "\n",
      "Epoch 136/199\n",
      "----------\n",
      "train Loss: 0.0424 Acc: 0.9921\n",
      "test Loss: 0.1874 Acc: 0.9426\n",
      "\n",
      "Epoch 137/199\n",
      "----------\n",
      "train Loss: 0.0614 Acc: 0.9817\n",
      "test Loss: 0.1532 Acc: 0.9514\n",
      "\n",
      "Epoch 138/199\n",
      "----------\n",
      "train Loss: 0.0554 Acc: 0.9843\n",
      "test Loss: 0.1696 Acc: 0.9558\n",
      "\n",
      "Epoch 139/199\n",
      "----------\n",
      "train Loss: 0.0547 Acc: 0.9856\n",
      "test Loss: 0.1857 Acc: 0.9492\n",
      "\n",
      "Epoch 140/199\n",
      "----------\n",
      "train Loss: 0.0426 Acc: 0.9895\n",
      "test Loss: 0.1999 Acc: 0.9382\n",
      "2019-05-24 21-14-30\n",
      "\n",
      "Epoch 141/199\n",
      "----------\n",
      "train Loss: 0.0479 Acc: 0.9908\n",
      "test Loss: 0.1608 Acc: 0.9492\n",
      "\n",
      "Epoch 142/199\n",
      "----------\n",
      "train Loss: 0.0482 Acc: 0.9882\n",
      "test Loss: 0.1728 Acc: 0.9470\n",
      "\n",
      "Epoch 143/199\n",
      "----------\n",
      "train Loss: 0.0484 Acc: 0.9817\n",
      "test Loss: 0.1624 Acc: 0.9514\n",
      "\n",
      "Epoch 144/199\n",
      "----------\n",
      "train Loss: 0.0667 Acc: 0.9817\n",
      "test Loss: 0.1826 Acc: 0.9360\n",
      "\n",
      "Epoch 145/199\n",
      "----------\n",
      "train Loss: 0.0788 Acc: 0.9777\n",
      "test Loss: 0.1704 Acc: 0.9536\n",
      "2019-05-24 21-14-55\n",
      "\n",
      "Epoch 146/199\n",
      "----------\n",
      "train Loss: 0.0758 Acc: 0.9803\n",
      "test Loss: 0.1519 Acc: 0.9625\n",
      "\n",
      "Epoch 147/199\n",
      "----------\n",
      "train Loss: 0.0637 Acc: 0.9790\n",
      "test Loss: 0.1731 Acc: 0.9470\n",
      "\n",
      "Epoch 148/199\n",
      "----------\n",
      "train Loss: 0.0598 Acc: 0.9830\n",
      "test Loss: 0.1700 Acc: 0.9492\n",
      "\n",
      "Epoch 149/199\n",
      "----------\n",
      "train Loss: 0.0378 Acc: 0.9934\n",
      "test Loss: 0.1588 Acc: 0.9581\n",
      "\n",
      "Epoch 150/199\n",
      "----------\n",
      "train Loss: 0.0694 Acc: 0.9817\n",
      "test Loss: 0.1565 Acc: 0.9581\n",
      "2019-05-24 21-15-20\n",
      "\n",
      "Epoch 151/199\n",
      "----------\n",
      "train Loss: 0.0492 Acc: 0.9843\n",
      "test Loss: 0.1888 Acc: 0.9404\n",
      "\n",
      "Epoch 152/199\n",
      "----------\n",
      "train Loss: 0.0502 Acc: 0.9830\n",
      "test Loss: 0.1937 Acc: 0.9382\n",
      "\n",
      "Epoch 153/199\n",
      "----------\n",
      "train Loss: 0.0444 Acc: 0.9882\n",
      "test Loss: 0.2002 Acc: 0.9338\n",
      "\n",
      "Epoch 154/199\n",
      "----------\n",
      "train Loss: 0.0492 Acc: 0.9882\n",
      "test Loss: 0.1972 Acc: 0.9470\n",
      "\n",
      "Epoch 155/199\n",
      "----------\n",
      "train Loss: 0.0490 Acc: 0.9882\n",
      "test Loss: 0.1914 Acc: 0.9426\n",
      "2019-05-24 21-15-45\n",
      "\n",
      "Epoch 156/199\n",
      "----------\n",
      "train Loss: 0.0473 Acc: 0.9843\n",
      "test Loss: 0.2107 Acc: 0.9382\n",
      "\n",
      "Epoch 157/199\n",
      "----------\n",
      "train Loss: 0.0537 Acc: 0.9843\n",
      "test Loss: 0.2097 Acc: 0.9470\n",
      "\n",
      "Epoch 158/199\n",
      "----------\n",
      "train Loss: 0.0607 Acc: 0.9843\n",
      "test Loss: 0.1917 Acc: 0.9426\n",
      "\n",
      "Epoch 159/199\n",
      "----------\n",
      "train Loss: 0.0615 Acc: 0.9817\n",
      "test Loss: 0.1840 Acc: 0.9514\n",
      "\n",
      "Epoch 160/199\n",
      "----------\n",
      "train Loss: 0.0506 Acc: 0.9803\n",
      "test Loss: 0.1976 Acc: 0.9426\n",
      "2019-05-24 21-16-10\n",
      "\n",
      "Epoch 161/199\n",
      "----------\n",
      "train Loss: 0.0773 Acc: 0.9830\n",
      "test Loss: 0.1965 Acc: 0.9470\n",
      "\n",
      "Epoch 162/199\n",
      "----------\n",
      "train Loss: 0.0502 Acc: 0.9869\n",
      "test Loss: 0.1772 Acc: 0.9470\n",
      "\n",
      "Epoch 163/199\n",
      "----------\n",
      "train Loss: 0.0703 Acc: 0.9803\n",
      "test Loss: 0.1675 Acc: 0.9514\n",
      "\n",
      "Epoch 164/199\n",
      "----------\n",
      "train Loss: 0.0546 Acc: 0.9843\n",
      "test Loss: 0.1837 Acc: 0.9470\n",
      "\n",
      "Epoch 165/199\n",
      "----------\n",
      "train Loss: 0.0821 Acc: 0.9751\n",
      "test Loss: 0.1821 Acc: 0.9448\n",
      "2019-05-24 21-16-35\n",
      "\n",
      "Epoch 166/199\n",
      "----------\n",
      "train Loss: 0.0425 Acc: 0.9856\n",
      "test Loss: 0.1774 Acc: 0.9426\n",
      "\n",
      "Epoch 167/199\n",
      "----------\n",
      "train Loss: 0.0643 Acc: 0.9830\n",
      "test Loss: 0.1716 Acc: 0.9448\n",
      "\n",
      "Epoch 168/199\n",
      "----------\n",
      "train Loss: 0.0811 Acc: 0.9699\n",
      "test Loss: 0.1790 Acc: 0.9514\n",
      "\n",
      "Epoch 169/199\n",
      "----------\n",
      "train Loss: 0.0601 Acc: 0.9856\n",
      "test Loss: 0.1824 Acc: 0.9514\n",
      "\n",
      "Epoch 170/199\n",
      "----------\n",
      "train Loss: 0.0475 Acc: 0.9869\n",
      "test Loss: 0.1664 Acc: 0.9492\n",
      "2019-05-24 21-16-57\n",
      "\n",
      "Epoch 171/199\n",
      "----------\n",
      "train Loss: 0.0640 Acc: 0.9843\n",
      "test Loss: 0.1619 Acc: 0.9536\n",
      "\n",
      "Epoch 172/199\n",
      "----------\n",
      "train Loss: 0.0284 Acc: 0.9948\n",
      "test Loss: 0.1669 Acc: 0.9492\n",
      "\n",
      "Epoch 173/199\n",
      "----------\n",
      "train Loss: 0.0520 Acc: 0.9895\n",
      "test Loss: 0.1669 Acc: 0.9558\n",
      "\n",
      "Epoch 174/199\n",
      "----------\n",
      "train Loss: 0.0473 Acc: 0.9869\n",
      "test Loss: 0.1687 Acc: 0.9426\n",
      "\n",
      "Epoch 175/199\n",
      "----------\n",
      "train Loss: 0.0707 Acc: 0.9790\n",
      "test Loss: 0.1640 Acc: 0.9514\n",
      "2019-05-24 21-17-20\n",
      "\n",
      "Epoch 176/199\n",
      "----------\n",
      "train Loss: 0.0447 Acc: 0.9895\n",
      "test Loss: 0.1763 Acc: 0.9536\n",
      "\n",
      "Epoch 177/199\n",
      "----------\n",
      "train Loss: 0.0597 Acc: 0.9843\n",
      "test Loss: 0.1852 Acc: 0.9558\n",
      "\n",
      "Epoch 178/199\n",
      "----------\n",
      "train Loss: 0.0548 Acc: 0.9843\n",
      "test Loss: 0.1785 Acc: 0.9492\n",
      "\n",
      "Epoch 179/199\n",
      "----------\n",
      "train Loss: 0.0603 Acc: 0.9843\n",
      "test Loss: 0.1626 Acc: 0.9470\n",
      "\n",
      "Epoch 180/199\n",
      "----------\n",
      "train Loss: 0.0699 Acc: 0.9751\n",
      "test Loss: 0.1597 Acc: 0.9426\n",
      "2019-05-24 21-17-44\n",
      "\n",
      "Epoch 181/199\n",
      "----------\n",
      "train Loss: 0.0735 Acc: 0.9777\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1049fdbaecfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSummaryWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_inception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"inception\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# model_ft = predict_model(model_ft, dataloaders_dict, criterion, optimizer_ft, is_inception=(model_name==\"inception\"))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9cf344bbf876>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloaders, criterion, optimizer, num_epochs, is_inception, save_interval)\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torchvision/models/squeezenet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         return torch.cat([\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand3x3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         ], 1)\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 320\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate\n",
    "writer=SummaryWriter()\n",
    "model_ft, m_hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n",
    "# model_ft = predict_model(model_ft, dataloaders_dict, criterion, optimizer_ft, is_inception=(model_name==\"inception\"))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint(os.path.join(chk_dir, 'saved'),\n",
    "               metric_dict={\n",
    "                   'epoch': model_hist['train'][-1][0],\n",
    "                   'type': model_name\n",
    "                   'train_acc': model_hist['train'][-1][2],\n",
    "                   'test_acc': model_hist['test'][-1][2],\n",
    "                   'model_dict': model_ft.state_dict()\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "    \n",
    "dict = testloader.dataset.class_to_idx\n",
    "test_class_dict = {value: key for key, value in dict.items()}\n",
    "test_class_dict\n",
    "for inputs, labels, paths in dataloaders_dict['test']:\n",
    "    labels = int(labels.cpu().numpy())\n",
    "    if labels in test_class_dict:\n",
    "        plt.figure(labels, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
